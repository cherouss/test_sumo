{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import optparse\n",
    "import random\n",
    "from itertools import  cycle\n",
    "import numpy as np\n",
    "from agent.agent import Agent\n",
    "from algos.ppo import *\n",
    "import time\n",
    "\n",
    "# we need to import python modules from the $SUMO_HOME/tools directory\n",
    "# i adden two hidden layers in ppo, if you have an error you know wht to do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "yes\n",
      "0.002 (0.9, 0.999)\n",
      "0.002\n",
      "0.002\n",
      "907200.0\n",
      "########## Solved! ##########\n",
      "907200.0\n",
      "597554.0\n",
      "########## Solved! ##########\n",
      "597554.0\n",
      "784170.0\n",
      "715699.0\n",
      "690482.0\n",
      "715021.0\n",
      "660754.0\n",
      "658777.0\n",
      "661068.0\n",
      "810796.0\n",
      "Episode 10 \t avg length: 556 \t reward: -74182\n",
      "844566.0\n",
      "668435.0\n",
      "646128.0\n",
      "628159.0\n",
      "656766.0\n",
      "726751.0\n",
      "786360.0\n",
      "728475.0\n",
      "946625.0\n",
      "882135.0\n",
      "Episode 20 \t avg length: 555 \t reward: -75236\n",
      "826177.0\n",
      "861470.0\n",
      "1010683.0\n",
      "731102.0\n",
      "1113575.0\n",
      "972400.0\n",
      "814999.0\n",
      "1013980.0\n",
      "750912.0\n",
      "984171.0\n",
      "Episode 30 \t avg length: 559 \t reward: -75253\n",
      "768934.0\n",
      "923379.0\n",
      "865616.0\n",
      "808011.0\n",
      "737108.0\n",
      "886268.0\n",
      "1031701.0\n",
      "1042509.0\n",
      "1288053.0\n",
      "1113342.0\n",
      "Episode 40 \t avg length: 557 \t reward: -77784\n",
      "1141902.0\n",
      "949996.0\n",
      "1063288.0\n",
      "1060807.0\n",
      "1050611.0\n",
      "1438990.0\n",
      "1046475.0\n",
      "1027998.0\n",
      "915683.0\n",
      "1219164.0\n",
      "Episode 50 \t avg length: 566 \t reward: -80024\n",
      "1080375.0\n",
      "1218905.0\n",
      "918535.0\n",
      "1060537.0\n",
      "782532.0\n",
      "705844.0\n",
      "713198.0\n",
      "852922.0\n",
      "587145.0\n",
      "########## Solved! ##########\n",
      "587145.0\n",
      "878716.0\n",
      "Episode 60 \t avg length: 559 \t reward: -74267\n",
      "778830.0\n",
      "751529.0\n",
      "791091.0\n",
      "869087.0\n",
      "890950.0\n",
      "877409.0\n",
      "846541.0\n",
      "689084.0\n",
      "899138.0\n",
      "836320.0\n",
      "Episode 70 \t avg length: 560 \t reward: -72075\n",
      "861730.0\n",
      "1113627.0\n",
      "892693.0\n",
      "982442.0\n",
      "1804202.0\n",
      "1678945.0\n",
      "944791.0\n",
      "1344068.0\n",
      "1392019.0\n",
      "1309881.0\n",
      "Episode 80 \t avg length: 585 \t reward: -79250\n",
      "1376374.0\n",
      "1796167.0\n",
      "1253463.0\n",
      "1314736.0\n",
      "1104662.0\n",
      "992289.0\n",
      "1763977.0\n",
      "1176587.0\n",
      "1488967.0\n",
      "1488593.0\n",
      "Episode 90 \t avg length: 589 \t reward: -80583\n",
      "1352015.0\n",
      "1079138.0\n",
      "1293966.0\n",
      "1653862.0\n",
      "1136332.0\n",
      "1705651.0\n",
      "1316819.0\n",
      "1158459.0\n",
      "1674429.0\n",
      "1397489.0\n",
      "Episode 100 \t avg length: 583 \t reward: -84014\n",
      "1715823.0\n",
      "1224399.0\n",
      "1694194.0\n",
      "1411277.0\n",
      "1209162.0\n",
      "1347357.0\n",
      "1236580.0\n",
      "1475344.0\n",
      "1154642.0\n",
      "1210785.0\n",
      "Episode 110 \t avg length: 581 \t reward: -83193\n",
      "1648144.0\n",
      "894812.0\n",
      "1493858.0\n",
      "1257263.0\n",
      "1193505.0\n",
      "1153249.0\n",
      "1018286.0\n",
      "1314264.0\n",
      "1081867.0\n",
      "1175490.0\n",
      "Episode 120 \t avg length: 587 \t reward: -78396\n",
      "902147.0\n",
      "687656.0\n",
      "1061673.0\n",
      "861730.0\n",
      "837611.0\n",
      "925114.0\n",
      "893811.0\n",
      "831909.0\n",
      "1223443.0\n",
      "810685.0\n",
      "Episode 130 \t avg length: 573 \t reward: -72060\n",
      "856273.0\n",
      "841013.0\n",
      "919395.0\n",
      "783073.0\n",
      "977252.0\n",
      "976566.0\n",
      "753829.0\n",
      "873475.0\n",
      "808020.0\n",
      "1029970.0\n",
      "Episode 140 \t avg length: 565 \t reward: -71914\n",
      "906997.0\n",
      "914755.0\n",
      "1475361.0\n",
      "1196684.0\n",
      "1425091.0\n",
      "1090119.0\n",
      "1059320.0\n",
      "1162031.0\n",
      "1378974.0\n",
      "845206.0\n",
      "Episode 150 \t avg length: 585 \t reward: -76881\n",
      "1132605.0\n",
      "1744042.0\n",
      "1099484.0\n",
      "992692.0\n",
      "1050019.0\n",
      "915361.0\n",
      "1355923.0\n",
      "1419145.0\n",
      "1347315.0\n",
      "1426634.0\n",
      "Episode 160 \t avg length: 591 \t reward: -76603\n",
      "1559909.0\n",
      "863260.0\n",
      "1672522.0\n",
      "1040313.0\n",
      "886272.0\n",
      "1290517.0\n",
      "849451.0\n",
      "1235131.0\n",
      "1599359.0\n",
      "1303343.0\n",
      "Episode 170 \t avg length: 592 \t reward: -78251\n",
      "1093165.0\n",
      "1152858.0\n",
      "1236812.0\n",
      "878826.0\n",
      "1201346.0\n",
      "1265528.0\n",
      "1165997.0\n",
      "1132553.0\n",
      "966256.0\n",
      "1254788.0\n",
      "Episode 180 \t avg length: 587 \t reward: -76265\n",
      "911643.0\n",
      "1046032.0\n",
      "974526.0\n",
      "870699.0\n",
      "1038913.0\n",
      "968711.0\n",
      "1471836.0\n",
      "656340.0\n",
      "844546.0\n",
      "985221.0\n",
      "Episode 190 \t avg length: 583 \t reward: -72339\n",
      "988186.0\n",
      "719035.0\n",
      "708378.0\n",
      "800763.0\n",
      "1227058.0\n",
      "798389.0\n",
      "765154.0\n",
      "768145.0\n",
      "853918.0\n",
      "710435.0\n",
      "Episode 200 \t avg length: 576 \t reward: -71728\n",
      "995389.0\n",
      "683740.0\n",
      "866156.0\n",
      "534463.0\n",
      "########## Solved! ##########\n",
      "534463.0\n",
      "1261525.0\n",
      "1301348.0\n",
      "868423.0\n",
      "693959.0\n",
      "1332079.0\n",
      "828789.0\n",
      "Episode 210 \t avg length: 582 \t reward: -73169\n",
      "818094.0\n",
      "973399.0\n",
      "1099456.0\n",
      "1082911.0\n",
      "1065669.0\n",
      "824224.0\n",
      "1216378.0\n",
      "725798.0\n",
      "1023248.0\n",
      "758020.0\n",
      "Episode 220 \t avg length: 589 \t reward: -71054\n",
      "705571.0\n",
      "663005.0\n",
      "855687.0\n",
      "717454.0\n",
      "599059.0\n",
      "686238.0\n",
      "980972.0\n",
      "965674.0\n",
      "802061.0\n",
      "1310175.0\n",
      "Episode 230 \t avg length: 584 \t reward: -72140\n",
      "1201773.0\n",
      "1081455.0\n",
      "935967.0\n",
      "743023.0\n",
      "815346.0\n",
      "982312.0\n",
      "820852.0\n",
      "1383390.0\n",
      "644794.0\n",
      "752702.0\n",
      "Episode 240 \t avg length: 590 \t reward: -70831\n",
      "682109.0\n",
      "1299009.0\n",
      "551652.0\n",
      "1000558.0\n",
      "1032790.0\n",
      "1149025.0\n",
      "925762.0\n",
      "616443.0\n",
      "930856.0\n",
      "1223799.0\n",
      "Episode 250 \t avg length: 592 \t reward: -70514\n",
      "1208754.0\n",
      "764819.0\n",
      "1323523.0\n",
      "870648.0\n",
      "1075011.0\n",
      "983491.0\n",
      "817167.0\n",
      "1002193.0\n",
      "1249197.0\n",
      "575429.0\n",
      "Episode 260 \t avg length: 593 \t reward: -71821\n",
      "570961.0\n",
      "740543.0\n",
      "979319.0\n",
      "1179067.0\n",
      "748302.0\n",
      "1121945.0\n",
      "1066533.0\n",
      "705548.0\n",
      "805344.0\n",
      "854835.0\n",
      "Episode 270 \t avg length: 588 \t reward: -69627\n",
      "971222.0\n",
      "1729062.0\n",
      "918495.0\n",
      "589861.0\n",
      "868241.0\n",
      "741281.0\n",
      "1373895.0\n",
      "846999.0\n",
      "1001244.0\n",
      "1349422.0\n",
      "Episode 280 \t avg length: 594 \t reward: -74273\n",
      "1103077.0\n",
      "644159.0\n",
      "1370344.0\n",
      "730171.0\n",
      "798707.0\n",
      "793956.0\n",
      "716129.0\n",
      "1780872.0\n",
      "1050574.0\n",
      "752820.0\n",
      "Episode 290 \t avg length: 589 \t reward: -74795\n",
      "961019.0\n",
      "571197.0\n",
      "867494.0\n",
      "1109019.0\n",
      "1183256.0\n",
      "1049966.0\n",
      "901813.0\n",
      "869637.0\n",
      "1306342.0\n",
      "1024622.0\n",
      "Episode 300 \t avg length: 597 \t reward: -70508\n",
      "820415.0\n",
      "1134377.0\n",
      "1007248.0\n",
      "918674.0\n",
      "1010375.0\n",
      "1330180.0\n",
      "890198.0\n",
      "1266172.0\n",
      "1293177.0\n",
      "845015.0\n",
      "Episode 310 \t avg length: 596 \t reward: -72355\n",
      "930082.0\n",
      "856422.0\n",
      "1378422.0\n",
      "1391340.0\n",
      "1047856.0\n",
      "1137721.0\n",
      "1417665.0\n",
      "791500.0\n",
      "701783.0\n",
      "929508.0\n",
      "Episode 320 \t avg length: 601 \t reward: -70778\n",
      "667386.0\n",
      "961622.0\n",
      "767490.0\n",
      "799241.0\n",
      "1343546.0\n",
      "1096189.0\n",
      "652941.0\n",
      "885335.0\n",
      "1028530.0\n",
      "757356.0\n",
      "Episode 330 \t avg length: 589 \t reward: -68321\n",
      "793516.0\n",
      "649159.0\n",
      "1326013.0\n",
      "754728.0\n",
      "1203769.0\n",
      "778671.0\n",
      "871889.0\n",
      "1239138.0\n",
      "764545.0\n",
      "577824.0\n",
      "Episode 340 \t avg length: 586 \t reward: -69398\n",
      "739475.0\n",
      "807445.0\n",
      "623745.0\n",
      "907302.0\n",
      "642328.0\n",
      "734747.0\n",
      "1216163.0\n",
      "785139.0\n",
      "923049.0\n",
      "773830.0\n",
      "Episode 350 \t avg length: 582 \t reward: -66655\n",
      "1043495.0\n",
      "850090.0\n",
      "769113.0\n",
      "814753.0\n",
      "825054.0\n",
      "687133.0\n",
      "782475.0\n",
      "895328.0\n",
      "967665.0\n",
      "1095405.0\n",
      "Episode 360 \t avg length: 595 \t reward: -66842\n",
      "1003062.0\n",
      "737225.0\n",
      "948357.0\n",
      "796289.0\n",
      "726628.0\n",
      "604476.0\n",
      "1000203.0\n",
      "986776.0\n",
      "550849.0\n",
      "624030.0\n",
      "Episode 370 \t avg length: 587 \t reward: -66308\n",
      "895678.0\n",
      "566385.0\n",
      "619525.0\n",
      "687350.0\n",
      "623448.0\n",
      "1063042.0\n",
      "706624.0\n",
      "782940.0\n",
      "634930.0\n",
      "779850.0\n",
      "Episode 380 \t avg length: 587 \t reward: -64893\n",
      "984466.0\n",
      "544488.0\n",
      "567145.0\n",
      "1086372.0\n",
      "718729.0\n",
      "699274.0\n",
      "779652.0\n",
      "741881.0\n",
      "592263.0\n",
      "1163009.0\n",
      "Episode 390 \t avg length: 584 \t reward: -66806\n",
      "666293.0\n",
      "852797.0\n",
      "839032.0\n",
      "669819.0\n",
      "875387.0\n",
      "733716.0\n",
      "750632.0\n",
      "869692.0\n",
      "625172.0\n",
      "810269.0\n",
      "Episode 400 \t avg length: 586 \t reward: -66063\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/NEW')\n",
    "\n",
    "his = []\n",
    "state_dim = 12\n",
    "action_dim = 2\n",
    "agent = Agent(act_dim=action_dim,state_dim=state_dim)\n",
    "\n",
    "render = False\n",
    "log_interval = 10           # print avg reward in the interval\n",
    "max_episodes = 600       # max training episodes\n",
    "max_timesteps = 3600         # max timesteps in one episode\n",
    "n_latent_var = 20           # number of variables in hidden layer\n",
    "update_timestep = 50000      # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 8               # update policy for K epochs\n",
    "eps_clip = 0.6          # clip parameter for PPO\n",
    "random_seed = 47\n",
    "    #############################################\n",
    "name = f'50n_2hidden_layersv3_{time.time()}'\n",
    "if random_seed:\n",
    "    torch.manual_seed(random_seed)\n",
    "        #env.seed(random_seed)\n",
    "    \n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "    \n",
    "    # logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "timestep = 0\n",
    "min_objective = float('inf')\n",
    "rewards = []\n",
    "    # training loop\n",
    "print(ppo.get_lr())\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    rewards = []\n",
    "    agent.terminate = False\n",
    "    \n",
    "    agent.reset()\n",
    "    state = agent.get_state()\n",
    "    #print(state)\n",
    "    agent.step()\n",
    "    \n",
    "    for t in range(max_timesteps):\n",
    "        agent.step()\n",
    "        timestep += 5\n",
    "            # Running policy_old:\n",
    "        action = ppo.policy_old.act(state, memory)\n",
    "        #print(action)\n",
    "        state, reward, done = agent.action(action)\n",
    "        rewards.append(reward)\n",
    "            # Saving reward and is_terminal:\n",
    "        #print(state, reward, done)\n",
    "        #print('reward ' + str(reward))\n",
    "        #if done:print('iiiiiiiiiiiiiiiii')\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "                    # update if its time\n",
    "        if timestep % update_timestep == 0:\n",
    "            #print('yes')\n",
    "            #print(reward, action)\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            timestep = 0\n",
    "        \n",
    "            #if render:\n",
    "             #   env.render()\n",
    "        if done:\n",
    "            #agent.reset()\n",
    "            break\n",
    "        running_reward += reward\n",
    "    his.append(agent.waiting_time)\n",
    "    #print(sum(rewards)/len(rewards))\n",
    "    writer.add_scalar(\"waiting_time/train\", agent.his[-1][0], i_episode)\n",
    "    writer.add_scalar(\"CO2\", agent.his[-1][1], i_episode)\n",
    "    writer.add_scalar(\"rewards\", sum(rewards)/len(rewards), i_episode)\n",
    "    #print(agent.waiting_time)\n",
    "    avg_length += t\n",
    "    \n",
    "        # stop training if avg_reward > solved_reward\n",
    "    if min_objective > agent.his[-1][0]:\n",
    "        print(\"########## Solved! ##########\")\n",
    "        min_objective = agent.his[-1][0]\n",
    "        print(min_objective)\n",
    "        torch.save(ppo.policy.state_dict(), f'models_save/PPO_{name}.pth')\n",
    "       # break\n",
    "            \n",
    "        # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = int(avg_length/log_interval)\n",
    "        running_reward = int((running_reward/log_interval))\n",
    "            \n",
    "        print(f'Episode {i_episode} \\t avg length: {avg_length} \\t reward: {running_reward}')\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(r'out\\here')\n",
    "\n",
    "writer.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "#print(agent.his)\n",
    "y = range(len(agent.his[::10]))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y, agent.his[::10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(np.array(agent.his))\n",
    "df.to_csv('historyv2.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = state_dim\n",
    "num_outputs = action_dim\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 15\n",
    "lr               = 3e-4\n",
    "num_steps        = 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []\n",
    "agent = Agent(act_dim=2,state_dim=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()\n",
    "state = agent.get_state()\n",
    "early_stop = False\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "    agent.reset()\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        \n",
    "        next_state, reward, done, _ = agent.action(action.cpu().numpy())\n",
    "        agent.step()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env() for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "            \n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = torch.tensor([1,2,3,np.nan])\n",
    "#torch.zeros_like(a)\n",
    "torch.where(torch.isnan(a), torch.zeros_like(a), a)\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in e:\\anaconda3\\lib\\site-packages (from tensorboard) (0.14.1)\n",
      "Requirement already satisfied: wheel>=0.26 in e:\\anaconda3\\lib\\site-packages (from tensorboard) (0.31.1)\n",
      "Requirement already satisfied: six>=1.10.0 in e:\\anaconda3\\lib\\site-packages (from tensorboard) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in e:\\anaconda3\\lib\\site-packages (from tensorboard) (1.14.3)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.35.0-cp36-cp36m-win_amd64.whl (3.0 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: importlib-metadata in e:\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard) (3.3.0)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.15.2-cp36-cp36m-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in e:\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in e:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.22)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in e:\\anaconda3\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in e:\\anaconda3\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard) (3.7.4.3)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-53.0.0-py3-none-any.whl (784 kB)\n",
      "Installing collected packages: setuptools, rsa, requests, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tensorboard\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "Successfully installed absl-py-0.11.0 cachetools-4.2.1 google-auth-1.27.0 google-auth-oauthlib-0.4.2 grpcio-1.35.0 markdown-3.3.3 oauthlib-3.1.0 protobuf-3.15.2 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-53.0.0 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "twisted 20.3.0 requires attrs>=19.2.0, but you have attrs 18.1.0 which is incompatible.\n",
      "WARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'e:\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traci.constants as tc\n",
    "first = False\n",
    "def run():\n",
    "    \"\"\"execute the TraCI control loop\"\"\"\n",
    "    step = 0\n",
    "    test = 0\n",
    "    s = 0\n",
    "    #network.write(\"data/cross.net.xml\")\n",
    "    #print(type(traci.trafficlight.getCompleteRedYellowGreenDefinition('0')[0].getPhases()[0]))\n",
    "    lst = traci.trafficlight.getIDList()\n",
    "    #print(lst)\n",
    "    #traci.trafficlight.getPhase()\n",
    "    \n",
    "    for i in lst:\n",
    "        logic = traci.trafficlight.getAllProgramLogics(i)\n",
    "        print(logic)\n",
    "        \n",
    "    \n",
    "    while traci.simulation.getMinExpectedNumber() > 0:\n",
    "        traci.simulationStep()\n",
    "        test+= 1\n",
    "        #print(traci.vehicle.getAccumulatedWaitingTime)\n",
    "        for v in traci.vehicle.getIDList():\n",
    "            s+=traci.vehicle.getWaitingTime(v)\n",
    "            #traci.simulation.del\n",
    "        step += 1\n",
    "        lane_lst = traci.lanearea.getIDList()\n",
    "        #print(lane_lst)\n",
    "        act = 1\n",
    "        if step%50==0:\n",
    "            waiting_time,carbon_omission = agent.get_reward(traci)\n",
    "            print(agent.get_reward(traci))\n",
    "            print(agent.get_state(traci))\n",
    "            state = agent.get_state(traci)\n",
    "            i = 0\n",
    "            for info in state:\n",
    "                if info[0] > 10:\n",
    "                    print('changed')\n",
    "                    act = 0\n",
    "\n",
    "\n",
    "        \n",
    "        #if step % 20 == 0 :\n",
    "         #   act = 0\n",
    "        agent.action(act,traci)\n",
    "        if step >=200:\n",
    "            break\n",
    "    print(s)    \n",
    "    traci.close()\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def get_options():\n",
    "    optParser = optparse.OptionParser()\n",
    "    optParser.add_option(\"--nogui\", action=\"store_true\",\n",
    "                         default=False, help=\"run the commandline version of sumo\")\n",
    "    options, args = optParser.parse_args()\n",
    "    return options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(Logic(programID='0', type=0, currentPhaseIndex=0, phases=(Phase(duration=25.0, state='GrGr', minDur=25.0, maxDur=25.0, next=()), Phase(duration=20.0, state='yryr', minDur=20.0, maxDur=20.0, next=()), Phase(duration=42.0, state='rGrG', minDur=42.0, maxDur=42.0, next=()), Phase(duration=24.0, state='ryry', minDur=24.0, maxDur=24.0, next=())), subParameter={}),)\n",
      "(0.0, 23248.176340314367)\n",
      "[(3, 11.109450591881947), (2, 16.204270759607173), (0, -1.0), (0, 10.869061315818856)]\n",
      "(262.0, 45949.49188311631)\n",
      "[(7, 4.782935074153847), (5, 3.296839882560074), (0, -1.0), (0, -1.0)]\n",
      "(950.0, 68307.8137217103)\n",
      "[(11, 1.8405738498513542), (9, 0.0), (0, -1.0), (0, -1.0)]\n",
      "changed\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FatalTraCIError",
     "evalue": "connection closed by SUMO",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c83f69acb80e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-477f746e20c8>\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mtraci\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMinExpectedNumber\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mtraci\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#print(traci.vehicle.getAccumulatedWaitingTime)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py\u001b[0m in \u001b[0;36msimulationStep\u001b[1;34m(step)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0m_traceFile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_currentLabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"traci.simulationStep(%s)\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_connections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36msimulationStep\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mint\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"API change now handles step as floating point seconds\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sendCmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCMD_SIMSTEP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msubscriptionResults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_subscriptionMapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0msubscriptionResults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36m_sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"!i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mobjID\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"latin1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpacked\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sendExact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_readSubscription\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36m_sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mFatalTraCIError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"connection closed by SUMO\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcommand\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"!BBB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFatalTraCIError\u001b[0m: connection closed by SUMO"
     ]
    }
   ],
   "source": [
    "#options = get_options()\n",
    "var = 0\n",
    "label = 0\n",
    "generate_routefile()\n",
    "done = False\n",
    "while not done:\n",
    "    try:\n",
    "        traci.start(['sumo-gui', \"-c\", \"data/new/cross.sumocfg\",\n",
    "        \"--tripinfo-output\", \"tripinfo.xml\"],label=str(label))\n",
    "        done = True\n",
    "    except:\n",
    "        label+=1\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ppo()\n",
    "agent = Agent()\n",
    "max_iter = 1000\n",
    "while i < max_iter and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = agent.act(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        i += 1\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            test_reward = np.mean([test_env() for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "            \n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('e2det_1i_0', 'e2det_2i_0', 'e2det_3i_0', 'e2det_4i_0')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "traci.lanearea.getIDList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cy = cycle((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "a = [0]\n",
    "a[-1]\n",
    "with open(\"data/cross.rou.xml\", \"w\") as routes:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.7.1+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-1.7.1%2Bcpu-cp36-cp36m-win_amd64.whl (184.2 MB)\n",
      "Requirement already satisfied: dataclasses in e:\\anaconda3\\lib\\site-packages (from torch==1.7.1+cpu) (0.8)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda3\\lib\\site-packages (from torch==1.7.1+cpu) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\lib\\site-packages (from torch==1.7.1+cpu) (1.14.3)\n",
      "Collecting torchaudio===0.7.2\n",
      "  Using cached https://download.pytorch.org/whl/torchaudio-0.7.2-cp36-none-win_amd64.whl (103 kB)\n",
      "Collecting torchvision==0.8.2+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.8.2%2Bcpu-cp36-cp36m-win_amd64.whl (805 kB)\n",
      "Requirement already satisfied: pillow>=4.1.1 in e:\\anaconda3\\lib\\site-packages (from torchvision==0.8.2+cpu) (5.1.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.7.1+cpu torchaudio-0.7.2 torchvision-0.8.2+cpu\n",
      "WARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'e:\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "range(0, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "range(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'source' is not recognized as an internal or external command,\noperable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!source activate e:/envirments/ml/Scripts/Activate.ps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'source' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"E:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"E:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"E:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"E:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 480, in start\n",
      "    return self.subapp.start()\n",
      "  File \"E:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelspec.py\", line 176, in start\n",
      "    prefix=opts.prefix, display_name=opts.display_name)\n",
      "  File \"E:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelspec.py\", line 133, in install\n",
      "    path, kernel_name=kernel_name, user=user, prefix=prefix)\n",
      "  File \"E:\\Anaconda3\\lib\\site-packages\\jupyter_client\\kernelspec.py\", line 320, in install_kernel_spec\n",
      "    raise ValueError(\"Invalid kernel name %r.  %s\" % (kernel_name, _kernel_name_description))\n",
      "ValueError: Invalid kernel name 'e:/envirments/ml/scripts/activate.ps1'.  Kernel names can only contain ASCII letters and numbers and these separators: - . _ (hyphen, period, and underscore).\n"
     ]
    }
   ],
   "source": [
    "!source activate source activate e:/envirments/ml/Scripts/python\n",
    "!python -m ipykernel install --user --name e:/envirments/ml/Scripts/Activate.ps1 --display-name \"Python (myenv)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=25, tm_hour=16, tm_min=1, tm_sec=17, tm_wday=3, tm_yday=56, tm_isdst=0)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "#traci.start()\n",
    "import time\n",
    "time.localtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}